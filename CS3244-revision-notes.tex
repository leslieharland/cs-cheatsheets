\documentclass[11pt]{article}
\usepackage{amsmath}
\begin{document}
\noindent
\subsection*{Some terms}
Collaborative filtering - collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users\\
Assumes that one person has same opinion as the other person
\\\\
Inductive learning Assumption - after seeing many training examples result in high accuracy on unobserved examples\\
Hypothesis approximates well\\
\\
average hypothesis only depends on H, does not always lie in H\\
$H_{2} \subset H_{10}$\\
\\
Target function y = f(x)\\
Unbiased learner cannot generalise \rightarrow\) inductive bias
\\\\
h is +ve when h(x) = 1
$\geq_{g}$ more general than	 or equal to\\
h_{j} $\geq_{g}$ h_{k} \equiv \forall x \in X (h_{k}(x) = 1) \rightarrow (h_{j}(x) = 1)\)
\\
Not a total order has there may have the same specificity but different hypothesis (maximally specific h)
\\
\\
Consistency: h is consistent iff h(x) = f(x)\\
 h4(\langle0,1\rangle) = 1 \neq f(\langle0,1\rangle) = 0.\\\\
 \textbf{Proposition 1} \\
\)h is consistent with ðƒ iff every +ve training instance satisfies h and every
-ve training instance does not satisfy h.\\
 \textbf{Proposition 2} \\
\)If h belongs to H, then it is consistent
\subsection*{Naive Bayes}\\
\subsubsection*{Bayes Rule}\\
There can be relatively lesser occurrences of one class
\\
prior odds / likelihood = posterior odds\\
Bayesian thinking\\
How likely is something not going to work?
\\
Hold your beliefs, shift your beliefs as u encounter with the world\\
$P(y | x) = \frac{P(x | y) \times P(y)}{P(x)}$
\\
P(X_{1} = 0, X_{2} = -1) = P(X_{1} = 0 \;|\ C_{1}) \times P(X_{2} = -1 \;|\ C_{1}) \times P(C_{1})
+ P(X_{1} = 0 \;|\ C_{2}) \times P(X_{2} = -1 \;|\ C_{2}) \times P(C_{2})
\)
\\\\
\textbf{a priori} - equally probable\\\\\\
Naive - Assuming that features are conditionally independent of each other
\\\\
Insensitive to the number of training examples
\\\\
Calculate which is more likely\\
#1. $P(Outcome | x)$ = P(X_{1} | Outcome) * P(X_{2}| Outcome) * P(Outcome)\\
#2. \)$P(Not\ Outcome | x)$ = P(X_{1}| Not\ Outcome) * P(X_{2}| Not\ Outcome) * P(Not\ Outcome)
\)
\\\\\\
Calculate probability outcome occurs\\
\#1/(\#1+\#2)
\subsection*{Perceptron classifier}\\
x_{0}\theta_{0} + x_{1} \theta_{1} + x_{2}\theta_{2}\\\) of the previous weight vector
\\
first instance will compute based on initial vector\\
For -1, first point checks == -1, != 1 for the points after\\
Update weight = - (x \times\) learning\_rate) for the misclassified instance\\
\\\)
For +1, first point checks != -1, == 1 for the points after\\
Update weight =  + (x \times\) learning\_rate) for the misclassified instance\\
\\
points are linearly separable if there are no longer any misclassified points
not misclassified: sign(x) function having -1 when x is \leq 0
\\\\
\subsection*{Supervised learning}\\
Identifying objects \\
Output would be a continuous value\\
Given many emails, you want to \textbf{determine} if they are Spam or Non-Spam emails.
\\
\\
Given historical weather records, \textbf{predict} if tomorrow's weather will be sunny or rainy.
\\\\
\subsection*{Unsupervised learning}\\
No labels (no output)
Kmeans 
\\
Input: Given a set of data points P, number of clusters k
\\1. Randomly pick k points from P as centers $c_{j} = 1.. k$
 \\2.
Iterate (until max-iterations or $c_{j}$ no longer changes up to some threshold) \\
- Assign each point to nearest center: $y_{i}$ = arg min {\parallel p_{i} - c_{j} \parallel}^{2}\)
\\
$c_{j}$ =  Re-estimate each center as mean of points assigned to it
\\\\
Given a set of news articles from many different news websites, find out what are the main topics covered.
\\\\
From the user usage patterns on a website, figure out what different groups of users exist.
\subsection*{RL}\\
Key usage: Planning problems, app placements product recommendations\\\\
\textbf{Goal}\\
Low risk when performing actions\\
Goal of markov property: operate in ease\\
choices that maximise the outcome\\
\\
Precision vs control\\
Decisions we make the optimize\\
\\
Prediction allows us to evaluate how good a policy is for a state space\\
Control\\
Reward at any step is optimal, highest reward
\\
continuous MDP - common in robots\\
No supervisor, only reward signal\\
Reward is a scalar feedback\\
whether one state vs the other is preferred\\
\\
RL Challenge: Does not have a good idea if it is right to do, which one is feasible, figuring out rules, which gives the best reward
\\
Given action will give observation and reward\\
Does not know the rules\\
\\
Planning Challenge:\\
A* search - Going through the search space to figure the appropriate action\\
what if i did this, what kind of consequences
\\
Rules are known\\
Uses a search space which requires tree search
\\
\\
Action affects the subsequent states\\
Action $\rightarrow$ Observation (what state it is in) $\rightarrow$  Reward\\
time t will not take the final reward but t - 1\\
Sometimes actions affect the observations\\
May contain irrelevant information of the state at time t\\
Although we have large field of vision, our neural cortex is trying to observe t changes in the environment\\
Function of history is not visible to the environment and is specific to agent\\
State - what information that we have in the brain is what we are going to need to predict the future\\
\\
\textbf{Fully observable}\\
The state can be represented different based on how we want the agent to see the sequence\\
congruent between the agent and the environment\\
\\
\textbf{Partially observable}\\
Align/ localize itself based on the map e.g. game
\\
Represented state can influence the agent
\\
Depending on the agent, we can end up in a different state\\
E.g. if we want to turn the rotor around the corner, it may turn out not to behave that way
\\
Recurrent neural networks\\
Using ReLu to construct each state in deep learning, easy to compute as no exponential operations
\subsubsection*{Policy}
Look through the mapping function\\
Deterministic $s_{1}$ to $a_{1}$ \\
Stochastic Take a particular action with a probability\\
\\
Value function - future reward\\
Sum of expected reward, reward farther in the future will have gamma to discount it\\
Model - probability that it will go to the next state, sum over all possible actions has to be 1
\\\\
\textbf{Agent taxonomy}\\
Value Based\\
Policy - in this state we take a particular action\\
Actor Critic - model based
\\
\subsection*{Exploration vs Exploitation}
Going to the restaurant that i like \\
Exploration - discover more information about the environment
\\
Exploitation - Exploits known information to maximise reward
\\\\
\subsection*{State transition probability matrix}\\
Represented with the symbol P\\
some of the values are 0 in the transition matrix if it cannot move to a particular state\\
episodes are finite\\
there are always probability attach to the state, eventually it will reach the terminal state
\subsection{Markov Reward process}
Attach the reward and discount\\
Differentiating reward,create two different states with different reward\\
$E(r_{t+1} | ..)$ immediately receive the reward in next time step
\\\\
Discount factor$\gamma $
\\ Near sighted - given where i am now where should I go
\\ Far sighted - sum all the rewards add all of them, favor longer samples (should have more reward)
\\
\textbf{Role of Discount}\\
Infinitely sample the cycle, a lot of uncertainties on the future
\subsection{Bellman equation}
Reward based on executing the state
\subsection*{Value iteration}\\
Idea of value iteration\\
Instead of looking what policy to take, look back and determine what is the best action brought us to get the best value function \\\\
Emits the observation and reward\\
Recover information or t and r\\
\\
Value iteration is a simple form of policy iteration\\
Use optimal V*  to create back the policy\\
\subsection*{Q learning}
Use q value to engineer the policy (monodically increasing)\\
r(s, a) reward at comes at next state\\
Higher value of q means we see it a lot of times, smooth it over time
\newpage\noindent
\subsection*{Neural Networks}
\textbf{Motivation}\\
In traditional ml, it is difficult to handcraft features, neural networks are good in feature learning\\\\
NN generalise badly\\
As the number of parameters (weights and layers) increase in a neural network, it can easily overfit the data.\\
Backpropogation is a complicated and tedious process. Moreover, the surface plotted by the error of a neural network in non-convex and high-dimensional and can consist of many hills and valleys - making it hard to reach the optimal set of weights.\\
\\
Do all the neurons in a layer always share the same bias value?
\\
Yes, that is correct.Â  The bias value is the same value ($x_0=1$) for all units at all layers, because we are going to learn the particular weight for the bias for each individual unit,Â $\theta_{0}$, separately.Â  It's just pictorially cleaner to think that each layer has a separate bias unit, but it doesn't matter, because the value of the bias unit is always (by convention) set to unity, such that the individual weights $\theta_{0}$ (or in the NN matrix notationÂ  $\theta^{(l)}_0$ are the controls for the biases.Â \\\\
\\
Random initialisation of weights for sign function\\
the sign() function takes different constant values in different regions and therefore has gradient 0.\\
the sign() function is discontinuous at x=0.\\
\\
Stack in together in a linear unit\\
Generalise things that are out of sample\\
\\
Regularisation for NN\\
Weight decay square of the weights are small so it is more toward linear regression (L2 regularisation) \\
\\
\\
\\
Is the Soft Weight Elimination function a Cost Function?\\
No, but it closely related.Â  It is a regulariser, a separate, artificial part of the augmented loss function, as what we learned in Week 05.Â  We denote regulariser functions as $\Omega(\theta)$.
\\\\
Would it have been a good idea to stop the first time the validation error goes up?\\
The validation error might fluctuate and therefore stopping it the first time it goes up will not give the optimal result. One can consider using a moving average or some other metric.\\
\subsection*{Deep Learning}
Capability to take in multiple input and give multiple outputs\\\\
Dropout - Generalise well rather than approximate well and overfit
ReLU - Vanishing gradients
\\
Bias- Variance tradeoff - Not generalizing due to too many parameters, model representation fit the particularities. Not fitting the additional noise.
\\
GAN create adversary to help u train better\\
E.g. mock test paper to do better in exam
\\
RNN time series inputs are correlated to each other\\
E.g. determiner followed by noun
\\
Weights of output from one node to another model layer of another time step is the same \\
\\
Model constraint input uses the same weights so that we can fewer parameters than 2
\\
\\
1 to Many: Image captioning\\
Many to 1: Sentiment Classification\\
Many to Many: NMachine translation
Many to Many (time sensitive): Stock market prediction\\
\\
\subsection*{RNN}
Tries to get the softmax chance higher
\textbf{Truncated backprop}\\
Instead of going to the start, backprop only to the start of the partition block
\subsection*{CNN}
Finding context where locality is important or somehow related to the things nearby\\
Key idea: Make it invariant to position\\
E.g. Non-linear transformation to find a horizontal lines\\
AND and OR gate to form a high level representation\\
\\
Image that maximises the strength of the corresponding units activation\\
\\
Tied samples over all the subsamples of the image\\
Sliding windows to build new feature maps\\
Sub-sampling (Pooling layers)\\
Stride - ignore intermediate squares
Max pooling gives the maximum of the units based on the filter size\\
\\
Layers at the beginning of the network has smaller activations as it is the problem of vanishing gradients (gradient is very small at end layers) stacking all of the layers together, when run on training (after pre-training)
\\
Layers get more positive or negative further down the network as compounding signals get more extreme, causing vanishing problem, doesn't give the same magnitude of gradient at different parts of the network
\\
- Learning rate has to compensate
- Weight decay regularisation 
\\ 
Or when values are close to 1, then we have issues with learning
\\
ReLu loses representation \\
\\
softmax cannot be used in nlp to predict next word as it can be computationally expensive
\\
hierarchical softmax can be computed in parallel
For unsupervised pre-training, weights by supervised learning and fine tuning it
\\
Final softmax layer needs supervision so we can only pre-train the middle layers
\\
GPU allows operation to happen simultaneously\\
each part of the GPU computing one feature map
\\
\\
performance will only get better for resnet with more layers as the residual will be learned by the next layer only if in the previous layer the residual is not captured.
\subsection*{Sparse Autoencoders}
Have to learn a smaller set of weights like a lossy compression algorithm \\
L1 to encourage sparsity
\subsection*{Dropout}
Idea: Random forest feature projection\\
Keep input fixed and only the output at other layers change\\
Rotate over multiple batches\\
Only certain neuron get the weight which can help in regularisation
\subsection*{Transfer learning}
Unsupervised feature extraction layers, or bottom and middle layers to use for new tasks\\
Hyperparameter to decide where to cut off softmax, FC, pooling
\\
Not useful for\\
VGG net has learned color or natural images that are not part of the domain
\subsection*{Representation Learning}
Uses embedding in the compressed layers
\subsection*{SVM}
Another linear unit
\newpage\noindent
\subsection*{Find S}
1.Initialise h to most specific h\\
2. If the positive instance differs, then replace with ?
\subsection*{Version space}
List all hypothesis, remove any hypothesis that is inconsistent with any training example
\subsection*{Candidate Elimination}
prefers positive examples over negatives as there can only be one maximally specific hypothesis in each iteration
? don't care\\
\empty no value\\
Set $S_{0}$ = \langle \emptyset \emptyset \emptyset \emptyset \emptyset \emptyset \rangle\\
\\
$G_{0}, G_{1}$ = \langle ? ? ? ? ? ? \rangle\\
\\
\)
For the negative example,\\
Create h from the specific hypothesis when it differs based on previous instance\\
Remove those that are inconsistent from the previous general hypotheses\\
Keep S to the previous instance S
\\\\
For the positive example,\\
replace with ? when it differs
\\
\subsection*{Cost functions}
Logistic regression\\
Is binary classification\\
Need meta strategies for multi class and it chooses the model that has the highest confidence in predictions
\\
Only gives values 0 and 1 so it is not a good result
\\\\
J(\theta) = \frac{1}{m} \sum\limits_{j = 1}^{m} ln(1 + exp^{-y^{{j}\theta^{T}x^{j}}})\\
\\\)\)
Linear regression\\
J(\theta) = \frac{1}{m} \sum\limits_{j = 1}^{m} (\theta^{T}x^{j} - y^{j})^2

\subsection*{Linear algebra}
trace(AB) = trace(BA) = trace(I_{n+1}) = n + 1\\
\\
Idempotency\\
H^2 = H\\
(I - H) ^2 = I - 2H + H = I - H\\
\\
\subsection*{Pseudoinverse}
(X^{T}X)^{-1} X^{T}\\
\theta = X^{+} y\\
X = \begin{pmatrix}
	1 &.. &..\\
	1 &.. &..\\
	1 &.. &..
	\end{pmatrix}
\\
\subsection*{Maximizing likelihood}
Minimize cross entropy = - $\frac{1}{m} \sum\limits_{j=1}^{m} ln\;g(y^{j} \theta^{T} x^{j})$
\subsection*{Ridge Regression}
$\frac{1}{m}\sum\limits_{i=1}^{m} (y_{i} - w^{T}x_{i})^2 + \lambda \parallel w \parallel ^2$
\\
$ -2 X^\top (Y - X * \theta) + 2 * \alpha * \theta $
\\
\subsection*{Noise}
Not deterministic only deterministic for f(x) = 0 \\
y - f(x)\\
$P(y | x)$\\
Noisy target = $E[y|x] + (y - f(x))$
\\
\subsubsection*{Stochastic noise vs Deterministic}

To reduce stochastic noise, the only way is to re-measure y
\\
To reduce deterministic noise, change H
\subsection*{Underfitting and Overfitting}
Why do we regularize?
We fit the data too much, we are fitting to the noise, balance fitting to the observed data. If we fit the data too well, the performance can be poor called overfitting. Overfitting can happen even if the data is not noisy.\\\\
\textbf{Overfit} - high level of noise, high complexity of f(x)\\
\textbf{Underfit} - high bias and low variance\\\\
Reduce bias by increasing complexity of hypotheses\\
simple representation of more complex reality
\subsection*{Non-linear transformation}
Transform this non-linear equation to a linear one: $y = ax^b$?
\\\\
 Under the condition $y>0$ and $x>0$
 \\
$ln(y)=ln(ax^b)$
\\
$ln(y)=ln(a)+ln(x^b)$
\\
$ln(y)=ln(a)+bln(x)$
\\
let $y'=ln(y)$, $x'=ln(x)$, $a'=ln(a)$, the equation can be written as $y'=a'+bx'$
\end{document}
